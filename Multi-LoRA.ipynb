{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lesson 6 - Multi-LoRA, Predibase, Inc, CTO Travis Adair\n",
    "# In this lesson, we'll see how to efficiently serve dozens of fine-tuned models together in a single deployment without sacrificing latency.\n",
    "\n",
    "# Import required packages\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Let's create a new model\n",
    "# We will start with creating an extension to the model from lesson 5. It has a custom helper function for computing the LoRA layer step with multiple LoRAs per batch.\n",
    "\n",
    "class AbstractMultiLoraModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # hidden_size = 10\n",
    "        # set this so low to ensure we are not \n",
    "        # compute-bound by the linear layer\n",
    "        # this is only an issue when running on CPU, \n",
    "        # for GPUs we can set this much\n",
    "        # higher and still avoid being compute bound\n",
    "        self.embedding = torch.nn.Embedding(10, 10)\n",
    "        self.linear = torch.nn.Linear(10, 10)\n",
    "        self.lm_head = torch.nn.Linear(10, 10)\n",
    "​\n",
    "    def linear_lora(\n",
    "        self,\n",
    "        x: torch.Tensor,                 # (batch_size, seq_len, in_features)\n",
    "        loras_a: torch.Tensor,           # (num_loras, in_features, rank)\n",
    "        loras_b: torch.Tensor,           # (num_loras, rank, out_features)\n",
    "        lora_indices: torch.LongTensor,  # (batch_size,)\n",
    "    ) -> torch.Tensor:\n",
    "        # y[i] = x[i] @ loras_a[lora_idx] @ loras_b[lora_idx]\n",
    "        raise NotImplementedError()\n",
    "​\n",
    "    def forward(self, input_ids, loras_a, loras_b, lora_indices):\n",
    "        x = self.embedding(input_ids)\n",
    "        x = self.linear_lora(x, loras_a, loras_b, lora_indices)\n",
    "        x = self.lm_head(x)\n",
    "        return x\n",
    "\n",
    "# Using a loop\n",
    "# Our first attempt to infer across multiple LoRAs will be straightforward: just loop over every row in the batch and apply the correct LoRA using an index mapping: batch_index --> lora_index.\n",
    "\n",
    "class LoopMultiLoraModel(AbstractMultiLoraModel):\n",
    "    def linear_lora(\n",
    "        self,\n",
    "        x: torch.Tensor,                 # (batch_size, seq_len, in_features)\n",
    "        loras_a: torch.Tensor,           # (num_loras, in_features, lora_rank)\n",
    "        loras_b: torch.Tensor,           # (num_loras, lora_rank, out_features)\n",
    "        lora_indices: torch.LongTensor,  # (batch_size,)\n",
    "    ) -> torch.Tensor:\n",
    "        y = self.linear(x)\n",
    "        for batch_idx, lora_idx in enumerate(lora_indices.numpy()):\n",
    "            lora_a = loras_a[lora_idx]\n",
    "            lora_b = loras_b[lora_idx]\n",
    "            y[batch_idx] += x[batch_idx] @ lora_a @ lora_b\n",
    "        return y\n",
    "# toy example of a detokenizer. The vocabular only consists of 10 words (different colors)\n",
    "detokenizer = [\n",
    "    \"red\",\n",
    "    \"orange\",\n",
    "    \"yellow\",\n",
    "    \"green\",\n",
    "    \"blue\",\n",
    "    \"indigo\",\n",
    "    \"violet\",\n",
    "    \"magenta\",\n",
    "    \"marigold\",\n",
    "    \"chartreuse\",\n",
    "]\n",
    "# dummy inputs\n",
    "input_ids = torch.LongTensor([[0, 1, 2, 3, 4, 5, 6, 7]])\n",
    "torch.manual_seed(42)\n",
    "def generate_token(model, **kwargs):\n",
    "    with torch.no_grad():\n",
    "        logits = model(**kwargs)\n",
    "    last_logits = logits[:, -1, :]\n",
    "    next_token_ids = last_logits.argmax(dim=1)\n",
    "​\n",
    "    return [detokenizer[token_id] for token_id in next_token_ids]\n",
    "model = LoopMultiLoraModel()\n",
    "\n",
    "# Let's try it!\n",
    "# We will try this over a few random LoRAs using a fixed tensor of input_ids. If our multi-LoRA generation process is working as designed, we should see a variety of different outputs generated as we randomly iterate over the LoRAs.\n",
    "\n",
    "# constants\n",
    "bs = 1\n",
    "num_loras = 64\n",
    "h = 10\n",
    "r = 2\n",
    "​\n",
    "# create contiguous blocks for 64 random LoRA weights\n",
    "loras_a = torch.randn(num_loras, h, r)\n",
    "loras_b = torch.randn(num_loras, r, h)\n",
    "​\n",
    "for i in range(10):\n",
    "    # randomize the LoRAs each iteration\n",
    "    lora_indices = torch.randint(num_loras, (bs,), dtype=torch.long)\n",
    "    next_token = generate_token(\n",
    "        model,\n",
    "        input_ids=input_ids,\n",
    "        loras_a=loras_a,\n",
    "        loras_b=loras_b,\n",
    "        lora_indices=lora_indices,\n",
    "    )\n",
    "    print(next_token)\n",
    "\n",
    "# Let's benchmark our multi-LoRA system!\n",
    "# We will measure the average latency to generate a single token as the batch size increases and each element within the batch can have a different LoRA adapter (chosen randomly).\n",
    "\n",
    "# constants\n",
    "seq_len = 8\n",
    "vocab_size = 10\n",
    "nsamples = 500\n",
    "max_batch_size = 64\n",
    "​\n",
    "​\n",
    "def benchmark(model):\n",
    "    avg_latencies = []\n",
    "    for bs in range(1, max_batch_size + 1):\n",
    "        latencies = []\n",
    "        for _ in range(nsamples):\n",
    "            # randomize the inputs and LoRA indices\n",
    "            input_ids = torch.randint(\n",
    "                vocab_size, (bs, seq_len), dtype=torch.long)\n",
    "            lora_indices = torch.randint(\n",
    "                num_loras, (bs,), dtype=torch.long)\n",
    "​\n",
    "            # measure the end-to-end latency for \n",
    "            # generating a single token\n",
    "            t0 = time.time()\n",
    "            next_token = generate_token(\n",
    "                model,\n",
    "                input_ids=input_ids,\n",
    "                loras_a=loras_a,\n",
    "                loras_b=loras_b,\n",
    "                lora_indices=lora_indices,\n",
    "            )\n",
    "            latencies.append(time.time() - t0)\n",
    "​\n",
    "        # average the latency across all the samples\n",
    "        latency_s = sum(latencies) / len(latencies)\n",
    "        avg_latencies.append(latency_s)\n",
    "        print(bs, latency_s)\n",
    "    return avg_latencies\n",
    "\n",
    "# Note: Your results might differ from those shown in the video, but they will still follow the same pattern.\n",
    "\n",
    "avg_latencies_loop = benchmark(model)\n",
    "\n",
    "# Let's visualize it!\n",
    "# Note: Your plot may vary slightly from the one shown in the video, yet it will exhibit a similar pattern.\n",
    "\n",
    "x = list(range(1, max_batch_size + 1))\n",
    "plt.plot(x, avg_latencies_loop, label=\"loop\")\n",
    "​\n",
    "plt.xlabel('Batch Size')\n",
    "plt.ylabel('Avg Latency (s)')\n",
    "plt.title('Multi-LoRA latency w.r.t. batch size')\n",
    "plt.legend()\n",
    "​\n",
    "plt.show()\n",
    "\n",
    "# Let's vectorize the LoRA computation\n",
    "# We will vectorize the LoRA computation by:\n",
    "\n",
    "# Gather the LoRA weight for each batch into a single tensor using torch.index_select.\n",
    "# Apply LoRA computation once for the entire input tensor.\n",
    "class GatheredMultiLoraModel(AbstractMultiLoraModel):\n",
    "    def linear_lora(\n",
    "        self,\n",
    "        x: torch.Tensor,                 # (batch_size, seq_len, in_features)\n",
    "        loras_a: torch.Tensor,           # (num_loras, in_features, lora_rank)\n",
    "        loras_b: torch.Tensor,           # (num_loras, lora_rank, out_features)\n",
    "        lora_indices: torch.LongTensor,  # (batch_size,)\n",
    "    ) -> torch.Tensor:\n",
    "        y = self.linear(x)\n",
    "        \n",
    "        # gather the LoRA weights into a new tensor and apply\n",
    "        lora_a = torch.index_select(loras_a, 0, lora_indices) # (batch_size, in_features, lora_rank)\n",
    "        lora_b = torch.index_select(loras_b, 0, lora_indices) # (batch_size, lora_rank, out_features)\n",
    "        y += x @ lora_a @ lora_b\n",
    "        return y\n",
    "model = GatheredMultiLoraModel()\n",
    "# Note: Your results might differ from those shown in the video, but they will still follow the same pattern.\n",
    "\n",
    "avg_latencies_gathered = benchmark(model)\n",
    "\n",
    "# Let's visualize it!\n",
    "# Note: Your plot may vary slightly from the one shown in the video, yet it will exhibit a similar pattern.\n",
    "\n",
    "x = list(range(1, max_batch_size + 1))\n",
    "plt.plot(x, avg_latencies_loop, label=\"loop\")\n",
    "plt.plot(x, avg_latencies_gathered, label=\"gathered\")\n",
    "​\n",
    "plt.xlabel('Batch Size')\n",
    "plt.ylabel('Avg Latency (s)')\n",
    "plt.title('Multi-LoRA latency w.r.t. batch size')\n",
    "plt.legend()\n",
    "​\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
