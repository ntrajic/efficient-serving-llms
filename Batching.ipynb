{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lesson 2 - Batching, Predibase, Inc, CTO Travis Adair\n",
    "# In this lesson, we'll discuss the concept of \"batching\" in LLM inference.\n",
    "\n",
    "# What is batching?\n",
    "# Throughput vs latency\n",
    "# Import required packages and load the LLM\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "model_name = \"./models/gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "# Reuse KV-cache text generation function from Lesson 1\n",
    "# Use the same prompt as the previous lesson to verify everything is working as expected\n",
    "prompt = \"The quick brown fox jumped over the\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "​\n",
    "​\n",
    "def generate_token_with_past(inputs):\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "​\n",
    "    logits = outputs.logits\n",
    "    last_logits = logits[0, -1, :]\n",
    "    next_token_id = last_logits.argmax()\n",
    "    return next_token_id, outputs.past_key_values\n",
    "​\n",
    "​\n",
    "def generate(inputs, max_tokens):\n",
    "    generated_tokens = []\n",
    "    next_inputs = inputs\n",
    "    for _ in range(max_tokens):\n",
    "        next_token_id, past_key_values = \\\n",
    "        generate_token_with_past(next_inputs)\n",
    "        next_inputs = {\n",
    "            \"input_ids\": next_token_id.reshape((1, 1)),\n",
    "            \"attention_mask\": torch.cat(\n",
    "                [next_inputs[\"attention_mask\"], torch.tensor([[1]])],\n",
    "                dim=1\n",
    "            ),\n",
    "            \"past_key_values\": past_key_values,\n",
    "        }\n",
    "​\n",
    "        next_token = tokenizer.decode(next_token_id)\n",
    "        generated_tokens.append(next_token)\n",
    "    return \"\".join(generated_tokens)\n",
    "​\n",
    "​\n",
    "tokens = generate(inputs, max_tokens=10)\n",
    "print(tokens)\n",
    "# Add padding tokens to the model to prepare batches of prompts\n",
    "# Define PAD Token = EOS Token = 50256\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.config.pad_token_id = model.config.eos_token_id\n",
    "# pad on the left so we can append new tokens on the right\n",
    "tokenizer.padding_side = \"left\"\n",
    "tokenizer.truncation_side = \"left\"\n",
    "Tokenize list of prompts\n",
    "# Add padding so that all prompts have the same number of tokens as the longest prompt\n",
    "# multiple prompts of varying lengths to send\n",
    "# to the model at once\n",
    "prompts = [\n",
    "    \"The quick brown fox jumped over the\",\n",
    "    \"The rain in Spain falls\",\n",
    "    \"What comes up must\",\n",
    "]\n",
    "​\n",
    "# note: padding=True ensures the padding token\n",
    "# will be inserted into the tokenized tensors\n",
    "inputs = tokenizer(prompts, padding=True, return_tensors=\"pt\")\n",
    "print(\"input_ids:\", inputs[\"input_ids\"])\n",
    "print(\"shape:\", inputs[\"input_ids\"].shape)\n",
    "print(\"attention_mask:\", inputs[\"attention_mask\"])\n",
    "print(\"shape:\", inputs[\"attention_mask\"].shape)\n",
    "# Add position ids to track original order of tokens in each prompt\n",
    "# Padding tokens are set to 1 and then first real token starts with position 0\n",
    "# position_ids tell the transformer the ordinal position\n",
    "# of each token in the input sequence\n",
    "# for single input inference, this is just [0 .. n]\n",
    "# for n tokens, but for batch inference,\n",
    "# we need to 0 out the padding tokens at the start of the sequence\n",
    "attention_mask = inputs[\"attention_mask\"]\n",
    "position_ids = attention_mask.long().cumsum(-1) - 1\n",
    "position_ids.masked_fill_(attention_mask == 0, 1)\n",
    "Pass tokens to model to calculate logits\n",
    "# same as before, but include the position_ids\n",
    "with torch.no_grad():\n",
    "    outputs = model(position_ids=position_ids, **inputs)\n",
    "logits = outputs.logits\n",
    "# Retrieve most likely token for each prompt\n",
    "last_logits = logits[:, -1, :] \n",
    "next_token_ids = last_logits.argmax(dim=1) \n",
    "# Print the next token ids\n",
    "print(next_token_ids)\n",
    "# Convert the token ids into strings\n",
    "next_tokens = tokenizer.batch_decode(next_token_ids)\n",
    "next_tokens\n",
    "Let's put it all together!\n",
    "# Generate n tokens with past\n",
    "def generate_batch_tokens_with_past(inputs):\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "​\n",
    "    logits = outputs.logits\n",
    "    last_logits = logits[:, -1, :]\n",
    "    next_token_ids = last_logits.argmax(dim=1)\n",
    "    return next_token_ids, outputs.past_key_values\n",
    "# Generate all tokens for some max tokens\n",
    "def generate_batch(inputs, max_tokens):\n",
    "    # create a list of tokens for every input in the batch\n",
    "    generated_tokens = [\n",
    "        [] for _ in range(inputs[\"input_ids\"].shape[0])\n",
    "    ]\n",
    "​\n",
    "    attention_mask = inputs[\"attention_mask\"]\n",
    "    position_ids = attention_mask.long().cumsum(-1) - 1\n",
    "    position_ids.masked_fill_(attention_mask == 0, 1)\n",
    "​\n",
    "    next_inputs = {\n",
    "        \"position_ids\": position_ids,\n",
    "        **inputs\n",
    "    }\n",
    "​\n",
    "    for _ in range(max_tokens):\n",
    "        next_token_ids, past_key_values = \\\n",
    "            generate_batch_tokens_with_past(next_inputs)\n",
    "​\n",
    "        next_inputs = {\n",
    "            \"input_ids\": next_token_ids.reshape((-1, 1)),\n",
    "            \"position_ids\": next_inputs[\"position_ids\"][:, -1].unsqueeze(-1) + 1,\n",
    "            \"attention_mask\": torch.cat([\n",
    "                next_inputs[\"attention_mask\"],\n",
    "                torch.ones((next_token_ids.shape[0], 1)),  \n",
    "            ], dim=1),\n",
    "            \"past_key_values\": past_key_values,\n",
    "        }\n",
    "​\n",
    "        next_tokens = tokenizer.batch_decode(next_token_ids)\n",
    "        for i, token in enumerate(next_tokens):\n",
    "            generated_tokens[i].append(token)\n",
    "    return [\"\".join(tokens) for tokens in generated_tokens]\n",
    "\n",
    "# Call the generate_batch function and print out the generated tokens\n",
    "generated_tokens = generate_batch(inputs, max_tokens=10)\n",
    "for prompt, generated in zip(prompts, generated_tokens):\n",
    "    print(prompt, f\"\\x1b[31m{generated}\\x1b[0m\\n\")\n",
    "\n",
    "# Throughput vs Latency\n",
    "# Explore the effect of batching on latency (how long it takes to generate each token).\n",
    "# Observe the fundamental tradeoff that exists between throughput and latency.\n",
    "# Note: Your results might differ somewhat from those shown in the video, but they will still follow the same pattern as explained by the instructor.\n",
    "\n",
    "# constants\n",
    "max_tokens = 10\n",
    "​\n",
    "# observations\n",
    "durations = []\n",
    "throughputs = []\n",
    "latencies = []\n",
    "​\n",
    "batch_sizes = [2**p for p in range(8)]\n",
    "for batch_size in batch_sizes:\n",
    "    print(f\"bs= {batch_size}\")\n",
    "​\n",
    "    # generate tokens for batch and record duration\n",
    "    t0 = time.time()\n",
    "    batch_prompts = [\n",
    "        prompts[i % len(prompts)] for i in range(batch_size)\n",
    "    ]\n",
    "    inputs = tokenizer(\n",
    "        batch_prompts, padding=True, return_tensors=\"pt\"\n",
    "    )\n",
    "    generated_tokens = generate_batch(inputs, max_tokens=max_tokens)\n",
    "    duration_s = time.time() - t0\n",
    "​\n",
    "    ntokens = batch_size * max_tokens\n",
    "    throughput = ntokens / duration_s\n",
    "    avg_latency = duration_s / max_tokens\n",
    "    print(\"duration\", duration_s)\n",
    "    print(\"throughput\", throughput)\n",
    "    print(\"avg latency\", avg_latency)    \n",
    "    print()\n",
    "​\n",
    "    durations.append(duration_s)\n",
    "    throughputs.append(throughput)\n",
    "    latencies.append(avg_latency)\n",
    "\n",
    "\n",
    "# Let's plot the throughput and latency observations against the batch size\n",
    "def render_plot(x, y1, y2, x_label, y1_label, y2_label):\n",
    "    # Create a figure and a set of subplots\n",
    "    fig, ax1 = plt.subplots()\n",
    "​\n",
    "    # Plot the first line (throughput)\n",
    "    color = 'tab:red'\n",
    "    ax1.set_xlabel(x_label)\n",
    "    ax1.set_ylabel(y1_label, color=color)\n",
    "    ax1.plot(x, y1, color=color)\n",
    "    ax1.tick_params(axis='y', labelcolor=color)\n",
    "​\n",
    "    # Set the x-axis to be log-scaled\n",
    "    ax1.set_xscale('log', base=2)\n",
    "​\n",
    "    # Instantiate a second axes that shares the same x-axis\n",
    "    ax2 = ax1.twinx()  \n",
    "    color = 'tab:blue'\n",
    "    ax2.set_ylabel(y2_label, color=color)  # we already handled the x-label with ax1\n",
    "    ax2.plot(x, y2, color=color)\n",
    "    ax2.tick_params(axis='y', labelcolor=color)\n",
    "​\n",
    "    plt.show()\n",
    "\n",
    "# Note: Your plot may vary slightly from the one shown in the video, yet it will exhibit a similar pattern.\n",
    "\n",
    "render_plot(\n",
    "    batch_sizes,\n",
    "    throughputs,\n",
    "    latencies,\n",
    "    \"Batch Size\",\n",
    "    \"Throughput\",\n",
    "    \"Latency\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
